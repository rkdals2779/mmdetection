#################### STEP: 0
---------- [__init__](0).positional args
@(/0) DINO(
  (data_preprocessor): DetDataPreprocessor()
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}
  (neck): ChannelMapper(
    (convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (extra_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (bbox_head): DINOHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=10, bias=True)
      (1): Linear(in_features=256, out_features=10, bias=True)
      (2): Linear(in_features=256, out_features=10, bias=True)
      (3): Linear(in_features=256, out_features=10, bias=True)
      (4): Linear(in_features=256, out_features=10, bias=True)
      (5): Linear(in_features=256, out_features=10, bias=True)
      (6): Linear(in_features=256, out_features=10, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (6): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=20, normalize=True, scale=6.283185307179586, eps=1e-06)
  (encoder): DeformableDetrTransformerEncoder(
    (layers): ModuleList(
      (0): DeformableDetrTransformerEncoderLayer(
        (self_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DeformableDetrTransformerEncoderLayer(
        (self_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DeformableDetrTransformerEncoderLayer(
        (self_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DeformableDetrTransformerEncoderLayer(
        (self_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DeformableDetrTransformerEncoderLayer(
        (self_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DeformableDetrTransformerEncoderLayer(
        (self_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DinoTransformerDecoder(
    (layers): ModuleList(
      (0): DeformableDetrTransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (cross_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DeformableDetrTransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (cross_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DeformableDetrTransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (cross_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DeformableDetrTransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (cross_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DeformableDetrTransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (cross_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DeformableDetrTransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (cross_attn): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn): FFN(
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=2048, out_features=256, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
          (gamma2): Identity()
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ref_point_head): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (query_embedding): Embedding(900, 256)
  (memory_trans_fc): Linear(in_features=256, out_features=256, bias=True)
  (memory_trans_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (dn_query_generator): CdnQueryGenerator(
    (label_embedding): Embedding(10, 256)
  )
)
---------- [__init__](0).keyword args
@(/num_queries) 900
@(/with_box_refine) True
@(/as_two_stage) True
@(/data_preprocessor/type) DetDataPreprocessor
@(/data_preprocessor/mean/0) 123.675
@(/data_preprocessor/mean/1) 116.28
@(/data_preprocessor/mean/2) 103.53
@(/data_preprocessor/std/0) 58.395
@(/data_preprocessor/std/1) 57.12
@(/data_preprocessor/std/2) 57.375
@(/data_preprocessor/bgr_to_rgb) True
@(/data_preprocessor/pad_size_divisor) 1
@(/backbone/type) ResNet
@(/backbone/depth) 50
@(/backbone/num_stages) 4
@(/backbone/out_indices/0) 1
@(/backbone/out_indices/1) 2
@(/backbone/out_indices/2) 3
@(/backbone/frozen_stages) 1
@(/backbone/norm_cfg/type) BN
@(/backbone/norm_cfg/requires_grad) False
@(/backbone/norm_eval) True
@(/backbone/style) pytorch
@(/backbone/init_cfg/type) Pretrained
@(/backbone/init_cfg/checkpoint) torchvision://resnet50
@(/neck/type) ChannelMapper
@(/neck/in_channels/0) 512
@(/neck/in_channels/1) 1024
@(/neck/in_channels/2) 2048
@(/neck/kernel_size) 1
@(/neck/out_channels) 256
@(/neck/act_cfg) None
@(/neck/norm_cfg/type) GN
@(/neck/norm_cfg/num_groups) 32
@(/neck/num_outs) 4
@(/encoder/num_layers) 6
@(/encoder/layer_cfg/self_attn_cfg/embed_dims) 256
@(/encoder/layer_cfg/self_attn_cfg/num_levels) 4
@(/encoder/layer_cfg/self_attn_cfg/dropout) 0.0
@(/encoder/layer_cfg/self_attn_cfg/batch_first) True
@(/encoder/layer_cfg/ffn_cfg/embed_dims) 256
@(/encoder/layer_cfg/ffn_cfg/feedforward_channels) 2048
@(/encoder/layer_cfg/ffn_cfg/ffn_drop) 0.0
@(/decoder/num_layers) 6
@(/decoder/return_intermediate) True
@(/decoder/layer_cfg/self_attn_cfg/embed_dims) 256
@(/decoder/layer_cfg/self_attn_cfg/num_heads) 8
@(/decoder/layer_cfg/self_attn_cfg/dropout) 0.0
@(/decoder/layer_cfg/self_attn_cfg/batch_first) True
@(/decoder/layer_cfg/cross_attn_cfg/embed_dims) 256
@(/decoder/layer_cfg/cross_attn_cfg/num_levels) 4
@(/decoder/layer_cfg/cross_attn_cfg/dropout) 0.0
@(/decoder/layer_cfg/cross_attn_cfg/batch_first) True
@(/decoder/layer_cfg/ffn_cfg/embed_dims) 256
@(/decoder/layer_cfg/ffn_cfg/feedforward_channels) 2048
@(/decoder/layer_cfg/ffn_cfg/ffn_drop) 0.0
@(/decoder/post_norm_cfg) None
@(/positional_encoding/num_feats) 128
@(/positional_encoding/normalize) True
@(/positional_encoding/offset) 0.0
@(/positional_encoding/temperature) 20
@(/bbox_head/type) DINOHead
@(/bbox_head/num_classes) 10
@(/bbox_head/sync_cls_avg_factor) True
@(/bbox_head/loss_cls/type) FocalLoss
@(/bbox_head/loss_cls/use_sigmoid) True
@(/bbox_head/loss_cls/gamma) 2.0
@(/bbox_head/loss_cls/alpha) 0.25
@(/bbox_head/loss_cls/loss_weight) 1.0
@(/bbox_head/loss_bbox/type) L1Loss
@(/bbox_head/loss_bbox/loss_weight) 5.0
@(/bbox_head/loss_iou/type) GIoULoss
@(/bbox_head/loss_iou/loss_weight) 2.0
@(/bbox_head/share_pred_layer) False
@(/bbox_head/num_pred_layer) 7
@(/bbox_head/as_two_stage) True
@(/bbox_head/train_cfg/assigner/type) HungarianAssigner
@(/bbox_head/train_cfg/assigner/match_costs/0/type) FocalLossCost
@(/bbox_head/train_cfg/assigner/match_costs/0/weight) 2.0
@(/bbox_head/train_cfg/assigner/match_costs/1/type) BBoxL1Cost
@(/bbox_head/train_cfg/assigner/match_costs/1/weight) 5.0
@(/bbox_head/train_cfg/assigner/match_costs/1/box_format) xywh
@(/bbox_head/train_cfg/assigner/match_costs/2/type) IoUCost
@(/bbox_head/train_cfg/assigner/match_costs/2/iou_mode) giou
@(/bbox_head/train_cfg/assigner/match_costs/2/weight) 2.0
@(/bbox_head/test_cfg/max_per_img) 300
@(/dn_cfg/label_noise_scale) 0.5
@(/dn_cfg/box_noise_scale) 1.0
@(/dn_cfg/group_cfg/dynamic) True
@(/dn_cfg/group_cfg/num_groups) None
@(/dn_cfg/group_cfg/num_dn_queries) 100
@(/dn_cfg/num_classes) 10
@(/dn_cfg/embed_dims) 256
@(/dn_cfg/num_matching_queries) 900
@(/train_cfg/assigner/type) HungarianAssigner
@(/train_cfg/assigner/match_costs/0/type) FocalLossCost
@(/train_cfg/assigner/match_costs/0/weight) 2.0
@(/train_cfg/assigner/match_costs/1/type) BBoxL1Cost
@(/train_cfg/assigner/match_costs/1/weight) 5.0
@(/train_cfg/assigner/match_costs/1/box_format) xywh
@(/train_cfg/assigner/match_costs/2/type) IoUCost
@(/train_cfg/assigner/match_costs/2/iou_mode) giou
@(/train_cfg/assigner/match_costs/2/weight) 2.0
@(/test_cfg/max_per_img) 300
---------- [__init__](0).outputs
@() None

